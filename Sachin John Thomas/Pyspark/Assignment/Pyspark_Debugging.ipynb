{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ecd12be-d671-4afb-a4a0-9461e60771ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.19.3.174:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>MySparkApp</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1bcfe20ffd0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"MySparkApp\").master(\"local[*]\").getOrCreate()\n",
    "from pyspark.sql.functions import col,count,avg, month, year, to_date, current_date, when\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c3ddf2-6b34-4303-a5a0-4087c8d04d96",
   "metadata": {},
   "source": [
    "1. Enable Spark Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a17d3d4-fe68-43fb-90d3-f95f9a210ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.setLogLevel(\"INFO\") # Options : All, DEBUG, INFO, WARN, ERROR\n",
    "# Helps reduce or increase log verbosity for better debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe35b1f-42fb-4b1c-8a48-7e7b7dd80fb5",
   "metadata": {},
   "source": [
    "2. Check Spark Configuration at Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "599d9921-3ee5-440b-8e0e-8466288d98a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.app.id', 'local-1747392811311'),\n",
       " ('spark.driver.port', '57770'),\n",
       " ('spark.driver.extraJavaOptions',\n",
       "  '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'),\n",
       " ('spark.sql.warehouse.dir',\n",
       "  'file:/C:/Users/Administrator/Big%20Data%20Class/Python/Pyspark/spark-warehouse'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.app.submitTime', '1747392809509'),\n",
       " ('spark.app.startTime', '1747392809713'),\n",
       " ('spark.driver.host', '172.19.3.174'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.executor.extraJavaOptions',\n",
       "  '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.app.name', 'MySparkApp'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.getConf().getAll()\n",
    "# USed to verify memory, cores, shuffle config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64ef800-2562-4c45-9194-f1a533bd48f2",
   "metadata": {},
   "source": [
    "3. Executor Memory Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b7eab91-090d-491f-893e-715b52db59cb",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid decimal literal (1039121640.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31m--conf spark.executor.memory = 2g\u001b[39m\n                                   ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid decimal literal\n"
     ]
    }
   ],
   "source": [
    "--conf spark.executor.memory = 2g\n",
    "--conf spark.driver.memory = 1g\n",
    "# Used when facing OutOfMemory errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df20c50c-4d3d-44de-afe9-fda012bc6406",
   "metadata": {},
   "source": [
    "4. Shuffle Debugging - Too much Shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee949eae-8d6d-48c9-92d4-53ad21c6f904",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m df = \u001b[43mdf\u001b[49m.repartition(\u001b[32m100\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mkey\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;66;03m# beofre groupBy or join\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Avoids default small partitions that cause skew\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df = df.repartition(100,\"key\") # beofre groupBy or join\n",
    "# Avoids default small partitions that cause skew"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100fd69b-f747-49db-a60b-f7273eb8fb57",
   "metadata": {},
   "source": [
    "5. Handling Skewed Join Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf358b84-2ae0-47b9-a6cd-36d494d7a964",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'big_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m broadcast\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m result = \u001b[43mbig_df\u001b[49m.join(broadcast(small_df),\u001b[33m\"\u001b[39m\u001b[33mkey\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m#Fixes long tasks dues to skew\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'big_df' is not defined"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import broadcast\n",
    "result = big_df.join(broadcast(small_df),\"key\")\n",
    "#Fixes long tasks dues to skew"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012b3173-c0f0-401a-b24a-821c28e2cb98",
   "metadata": {},
   "source": [
    "6. Catch Read Failures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9bf978e6-abda-4be8-9fe2-7c216fcbea4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to read :  [PATH_NOT_FOUND] Path does not exist: file:/C:/Users/Administrator/Big Data Class/Python/Pyspark/path/does/not/exist.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    df = spark.read.csv(\"path/does/not/exist\")\n",
    "except Exception as e:\n",
    "    print(\"Failed to read : \", e)\n",
    "# Good for detecting bad paths or file formats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d055e3-3690-47ac-af86-8be3d63103d3",
   "metadata": {},
   "source": [
    "7. Memory Usage via Executors tab\n",
    "\n",
    "Use Spark UI at localhost:4040 > Executors tab\n",
    "Diagnoses executor memory issues visually"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d188dd52-cb39-4d34-a6d1-a9ddac133aba",
   "metadata": {},
   "source": [
    "8. Monitor Job Execution Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a91c9c09-dcd6-43cc-b6ed-c96eac8943dc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtime\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m time \n\u001b[32m      2\u001b[39m start = time()\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mdf\u001b[49m.count()\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mExecution Time : \u001b[39m\u001b[33m\"\u001b[39m, time() - start)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Useful to measure slow operations\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "from time import time \n",
    "start = time()\n",
    "df.count()\n",
    "print(\"Execution Time : \", time() - start)\n",
    "# Useful to measure slow operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ed0077-04f2-4faa-b43f-91bed105d18c",
   "metadata": {},
   "source": [
    "9. Use .persist() Wisely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2bc7562c-17d3-4ccc-8e3e-f67b6325d136",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m df = \u001b[43mdf\u001b[49m.filter(\u001b[33m\"\u001b[39m\u001b[33mstatus = \u001b[39m\u001b[33m'\u001b[39m\u001b[33mactive\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m).persist()\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#Avoid caching raw or large unfiltered dataset\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df = df.filter(\"status = 'active'\").persist()\n",
    "#Avoid caching raw or large unfiltered dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d85b2a-5747-4356-9127-6a64b8590bc1",
   "metadata": {},
   "source": [
    "10. Analyze Task and Duration\n",
    "\n",
    "View task skew in Spark UI>Stages>Tasks\n",
    "\n",
    "Useful to detect long-runnning tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ed7f1d-0d98-488c-9152-686ba751da03",
   "metadata": {},
   "source": [
    "11. Avoid Exploding Memory in Collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "803d5d1a-5780-408a-81ca-b6c51d24b0b4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#Bad \u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m all_rows = \u001b[43mdf\u001b[49m.collect()\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Better\u001b[39;00m\n\u001b[32m      5\u001b[39m df.show(\u001b[32m10\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "#Bad \n",
    "all_rows = df.collect()\n",
    "\n",
    "# Better\n",
    "df.show(10)\n",
    "\n",
    "#Prevents memory issues on the driver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e48903f-239b-4288-baaf-c8521406453c",
   "metadata": {},
   "source": [
    "12. Check Number of Partitions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d0d2ec3-ca55-44f6-801b-be2529b13388",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdf\u001b[49m.rdd.getNUMPartitions())\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Too few partitions cause bottlenecks. Too many = overhead\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "print(df.rdd.getNUMPartitions())\n",
    "# Too few partitions cause bottlenecks. Too many = overhead"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8439f71b-9350-48af-880d-665c5ae5dd20",
   "metadata": {},
   "source": [
    "13. Debug Join Type Issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c5fe101f-f1db-4266-9cdc-94754bec8ddc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mdf1\u001b[49m.join(df2,\u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33minner\u001b[39m\u001b[33m\"\u001b[39m).explain(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Check for broadcast hint or sort-merge join issues\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'df1' is not defined"
     ]
    }
   ],
   "source": [
    "df1.join(df2,\"id\",\"inner\").explain(True)\n",
    "# Check for broadcast hint or sort-merge join issues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b49ac37-e2ae-4f54-915f-98b71a43e0cc",
   "metadata": {},
   "source": [
    "14. Investigate Lazy Evaluation Problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a7d0fdf3-2f0e-4004-8557-ed9cd414b0d4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mdf\u001b[49m.printSchema() \u001b[38;5;66;03m#Doesnt Trigger job\u001b[39;00m\n\u001b[32m      2\u001b[39m df.count() \u001b[38;5;66;03m#Triggers full execution\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Use actions to debug transformations\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.printSchema() #Doesnt Trigger job\n",
    "df.count() #Triggers full execution\n",
    "# Use actions to debug transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff84d0f-a76a-4523-bb64-7338d1f144f9",
   "metadata": {},
   "source": [
    "15. Track Failed Jobs\n",
    "\n",
    "Use Spark UI>Jobs tab>click failed job>review stderr logs\n",
    "\n",
    "Best for tracking OOM, file read, schema mismatch errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee0e1ac-9d83-4d5c-b861-e6c5a2a3ff01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
